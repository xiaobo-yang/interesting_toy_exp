
loading weights from pretrained gpt: gpt2
found 99 shards for split train
found 1 shards for split val
num decayed parameter tensors: 5, with 192,986,880 parameters
num non-decayed parameter tensors: 10, with 7,680 parameters
using fused AdamW: True
validation loss: 5.4144
Traceback (most recent call last):
  File "/data/my_scripts/interesting_toy_exp/multiple_generation_head/train_multiple_softmax.py", line 145, in <module>
    log_info.update({f"val_loss_layer_{layer}": val_losses_accum[layer].item() for layer in range(model.hook_layers)})
                                                                                                  ^^^^^^^^^^^^^^^^^
  File "/home/yangxiaobo/miniconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'DistributedDataParallel' object has no attribute 'hook_layers'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/my_scripts/interesting_toy_exp/multiple_generation_head/train_multiple_softmax.py", line 145, in <module>
[rank0]:     log_info.update({f"val_loss_layer_{layer}": val_losses_accum[layer].item() for layer in range(model.hook_layers)})
[rank0]:                                                                                                   ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/yangxiaobo/miniconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
[rank0]:     raise AttributeError(
[rank0]: AttributeError: 'DistributedDataParallel' object has no attribute 'hook_layers'